{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR53lePHuiP-"
   },
   "source": [
    "# Finetune PaliGemma\n",
    "\n",
    "> *These models and code are not official Google products and were trained and released for research purposes.*\n",
    "\n",
    "\n",
    "**This notebook shows how to finetune PaliGemma 2 on a vision-language task.**\n",
    "The training data consists of 90 pairs of images and long captions describing them.\n",
    "To make it runnable on a T4 colab runtime with 16GB HBM and 12GB RAM, we opt to only finetune the attention layers of the language model and freeze the other parameters.\n",
    "\n",
    " **This setup is illustrative**. In a real usecase, the amount of data, trainable parameters, training steps and hyper-parameters and obtained results could be significantly different.\n",
    "\n",
    "This notebook uses the model reference implementation from [big_vision](https://github.com/google-research/big_vision).\n",
    "and shows how to:\n",
    "\n",
    " * Install deps, download model checkpoint and training data.\n",
    " * Load the model onto GPU devices.\n",
    " * Prepare the input to the model for training and inference.\n",
    " * Finetune the model and inspect output in validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U0QUFveqSP2"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfxKb3F839Ks"
   },
   "outputs": [],
   "source": [
    "# @title Fetch big_vision code and install dependencies.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# TPUs with\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "  raise \"It seems you are using Colab with remote TPUs which is not supported.\"\n",
    "\n",
    "# Fetch big_vision repository if python doesn't know about it and install\n",
    "# dependencies needed for this notebook.\n",
    "!rm /home/dougljia@amd.com/big_vision/big_vision/configs/proj/paligemma/big_vision_repo -rf #Revise path on a different machine\n",
    "if not os.path.exists(\"big_vision_repo\"):\n",
    "  # Use local repository\n",
    "  !ln -s /home/dougljia@amd.com/big_vision big_vision_repo  ##Need to change this on a different machine\n",
    "\n",
    "# Append big_vision code to python import path\n",
    "if \"big_vision_repo\" not in sys.path:\n",
    "  sys.path.append(\"big_vision_repo\")\n",
    "\n",
    "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
    "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The T4 runtime is tight on memory to finetune this model. Preallocate\n",
    "# all memory ahead of time to avoid OOM'ing due to fragmentation.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n",
    "os.environ[\"NVTE_CK_USES_FWD_V3\"] = \"0\"\n",
    "os.environ[\"NVTE_CK_USES_BWD_V3\"] = \"1\"\n",
    "os.environ[\"NVTE_ALLOW_NONDETERMINISTIC_ALGO\"] = \"0\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "# Ensure deterministic behavior for multi-GPU communication (FSDP)\n",
    "os.environ[\"NCCL_DETERMINISTIC\"] = \"1\" \n",
    "os.environ[\"XLA_FLAGS\"] = (os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_deterministic_ops=true\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azmRZvgGyhAb"
   },
   "source": [
    "### Configure your API key to access Kaggle\n",
    "\n",
    "To use PaliGemma, you must provide your Kaggle username and a Kaggle API key.\n",
    "\n",
    "1. To generate a Kaggle API key, go to the **Account** tab of your Kaggle user profile and select **Create New Token**. This will trigger the download of a `kaggle.json` file containing your API credentials.\n",
    "1. In Colab, select **Secrets** (ðŸ”‘) in the left pane and add your Kaggle username and Kaggle API key. Store your username under the name `KAGGLE_USERNAME` and your API key under the name `KAGGLE_KEY`.\n",
    "\n",
    "To be able to download, you will also need to acknowledge the Terms and Conditions of the PaliGemma on:\n",
    "\n",
    "* https://www.kaggle.com/models/google/paligemma/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGLIp1Cx3_CX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import kagglehub\n",
    "\n",
    "# Use these for PaliGemma-2 3B 224pxÂ²\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "MODEL_PATH = \"./paligemma2-3b-pt-224.b16.npz\"\n",
    "KAGGLE_HANDLE = \"google/paligemma-2/jax/paligemma2-3b-pt-224\"  # Path to fetch from Kaggle.\n",
    "\n",
    "# Use these for PaliGemma 1:\n",
    "# LLM_VARIANT = \"gemma_2b\"\n",
    "# MODEL_PATH = \"./paligemma-3b-pt-224.f16.npz\"\n",
    "# KAGGLE_HANDLE = \"google/paligemma/jax/paligemma-3b-pt-224\"\n",
    "\n",
    "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
    "# vars as appropriate or make your credentials available in ~/.kaggle/kaggle.json\n",
    "\n",
    "# Securely prompt for credentials if not already in the environment; only need to run this once to download the model.\n",
    "if (not os.environ.get(\"KAGGLE_USERNAME\")) and (not os.path.exists(\"/root/.cache/kagglehub/models/google/paligemma-2/jax/paligemma2-3b-pt-224/1/./paligemma2-3b-pt-224.b16.npz\")):\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = getpass.getpass(\"Enter your Kaggle Username: \")\n",
    "\n",
    "if (not os.environ.get(\"KAGGLE_KEY\")) and (not os.path.exists(\"/root/.cache/kagglehub/models/google/paligemma-2/jax/paligemma2-3b-pt-224/1/./paligemma2-3b-pt-224.b16.npz\")):\n",
    "    os.environ[\"KAGGLE_KEY\"] = getpass.getpass(\"Enter your Kaggle Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQNOTfF24AV4",
    "outputId": "5241dd5b-d5c2-473c-a5e0-0ad72db288d8"
   },
   "outputs": [],
   "source": [
    "# @title Download checkpoint, tokenizer and dataset to local filesystem.\n",
    "#\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "  print(\"Downloading the checkpoint from Kaggle, this could take a few minutes....\")\n",
    "  MODEL_PATH = kagglehub.model_download(KAGGLE_HANDLE, MODEL_PATH)\n",
    "  print(f\"Model path: {MODEL_PATH}\")\n",
    "\n",
    "TOKENIZER_PATH = \"./paligemma_tokenizer.model\"\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "  print(\"Downloading the model tokenizer...\")\n",
    "  !gsutil cp gs://big_vision/paligemma_tokenizer.model {TOKENIZER_PATH}\n",
    "  print(f\"Tokenizer path: {TOKENIZER_PATH}\")\n",
    "\n",
    "DATA_DIR=\"./longcap100\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  print(\"Downloading the dataset...\")\n",
    "  !gsutil -m -q cp -n -r gs://longcap100/ .\n",
    "  print(f\"Data path: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDoq0O77GF30"
   },
   "source": [
    "## Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTfe2k8J4Bw0",
    "outputId": "51956b7f-8b7d-4565-cb11-1287595b054a"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import functools\n",
    "import html\n",
    "import io\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "# Import model definition from big_vision\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "from big_vision.trainers.proj.paligemma import predict_fns\n",
    "\n",
    "# Import big vision utilities\n",
    "import big_vision.datasets.jsonl\n",
    "import big_vision.utils\n",
    "import big_vision.sharding\n",
    "\n",
    "# Don't let TF use the GPU or TPUs\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "tf.config.set_visible_devices([], \"TPU\")\n",
    "\n",
    "backend = jax.extend.backend.get_backend()\n",
    "print(f\"JAX version:  {jax.__version__}\")\n",
    "print(f\"JAX platform: {backend.platform}\")\n",
    "print(f\"JAX devices:  {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aghcULcEdtv"
   },
   "outputs": [],
   "source": [
    "# @title Construct model and load params into RAM.\n",
    "\n",
    "# Define model\n",
    "# IMPORTANT: Gemma-2 has a \"final_logits_softcap\" property, we set it to 0.0\n",
    "# for better transfer results.\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\"vocab_size\": 257_152, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n",
    "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
    "})\n",
    "model = paligemma.Model(**model_config)\n",
    "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)\n",
    "\n",
    "# Load params - this can take up to 1 minute in T4 colabs.\n",
    "params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "# Define `decode` function to sample outputs from the model.\n",
    "decode_fn = predict_fns.get_all(model)['decode']\n",
    "decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWOdf_fw2SAO"
   },
   "outputs": [],
   "source": [
    "# @title Move params to GPU/TPU memory.\n",
    "#\n",
    "# To keep HBM usage low and fit in a T4 GPU (16GB HBM) we opt to only finetune\n",
    "# a part of the parameters. Additionally we keep the frozen params in float16\n",
    "# and cast trainable to float32.\n",
    "import fnmatch\n",
    "# Create a pytree mask of the trainable params.\n",
    "def is_trainable_param(name, param):  # pylint: disable=unused-argument\n",
    "  if name.startswith(\"llm/layers/attn/\"):  return True\n",
    "  # if name.startswith(\"llm/embedder\"):  return False\n",
    "  # if name.startswith(\"llm/layers/mlp\"):  return False\n",
    "  # if fnmatch.fnmatch(name, \"llm*norm*\"):              return False\n",
    "  if name.startswith(\"llm/\"):              return True\n",
    "  if name.startswith(\"img/Transformer/encoderblock/MultiHeadDotProductAttention\"):\n",
    "    return True\n",
    "  # if name.startswith(\"img/embedding/kernel\"):              return False\n",
    "  # if name.startswith(\"img/Transformer/encoderblock/MlpBlock\"):              return False\n",
    "  # if fnmatch.fnmatch(name, \"img/Transformer/enco*orm*\"):              return False\n",
    "  if name.startswith(\"img/\"):              return True\n",
    "  raise ValueError(f\"Unexpected param name {name}\")\n",
    "trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n",
    "\n",
    "#\n",
    "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
    "# be sharded across them to reduce HBM usage per device.\n",
    "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
    "\n",
    "data_sharding = jax.sharding.NamedSharding(\n",
    "    mesh, jax.sharding.PartitionSpec(\"data\"))\n",
    "\n",
    "params_sharding = big_vision.sharding.infer_sharding(\n",
    "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n",
    "\n",
    "# Yes: Some donated buffers are not usable.\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Some donated buffers were not usable\")\n",
    "\n",
    "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
    "def maybe_cast_to_f32(params, trainable):\n",
    "  # Cast others to bfloat16\n",
    "  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
    "                      if m else p.astype(jnp.bfloat16),\n",
    "                      params, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipJehqguO3T9",
    "outputId": "bbb5c58b-243d-4172-fb35-df9ff25c159b"
   },
   "outputs": [],
   "source": [
    "# Loading all params in simultaneous - albeit much faster and more succinct -\n",
    "# requires more RAM than the T4 colab runtimes have by default (12GB RAM).\n",
    "# Instead we do it param by param.\n",
    "params, treedef = jax.tree.flatten(params)\n",
    "sharding_leaves = jax.tree.leaves(params_sharding)\n",
    "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
    "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
    "  params[idx] = big_vision.utils.reshard(params[idx], sharding)\n",
    "  params[idx] = maybe_cast_to_f32(params[idx], trainable)\n",
    "  params[idx].block_until_ready()\n",
    "params = jax.tree.unflatten(treedef, params)\n",
    "\n",
    "# Print params to show what the model is made of.\n",
    "def parameter_overview(params):\n",
    "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
    "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
    "\n",
    "print(\" == Model params == \")\n",
    "parameter_overview(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SRW0NuU4UcW"
   },
   "outputs": [],
   "source": [
    "# @title Define preprocess functions to create inputs to the model.\n",
    "\n",
    "def preprocess_image(image, size=224):\n",
    "  # Model has been trained to handle images of different aspects ratios\n",
    "  # resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
    "  # options are helpful to improve quality in some tasks.\n",
    "  image = np.asarray(image)\n",
    "  if image.ndim == 2:  # Convert image without last channel into greyscale.\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "  image = image[..., :3]  # Remove alpha layer.\n",
    "  assert image.shape[-1] == 3\n",
    "\n",
    "  image = tf.constant(image)\n",
    "  image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
    "  return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
    "\n",
    "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
    "  # Model has been trained to handle tokenized text composed of a prefix with\n",
    "  # full attention and a suffix with causal attention.\n",
    "  separator = \"\\n\"\n",
    "  tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
    "  mask_ar = [0] * len(tokens)    # 0 to use full attention for prefix.\n",
    "  mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
    "\n",
    "  if suffix:\n",
    "    suffix = tokenizer.encode(suffix, add_eos=True)\n",
    "    tokens += suffix\n",
    "    mask_ar += [1] * len(suffix)    # 1 to use causal attention for suffix.\n",
    "    mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
    "\n",
    "  mask_input = [1] * len(tokens)    # 1 if its a token, 0 if padding.\n",
    "  if seqlen:\n",
    "    padding = [0] * max(0, seqlen - len(tokens))\n",
    "    tokens = tokens[:seqlen] + padding\n",
    "    mask_ar = mask_ar[:seqlen] + padding\n",
    "    mask_loss = mask_loss[:seqlen] + padding\n",
    "    mask_input = mask_input[:seqlen] + padding\n",
    "\n",
    "  return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
    "\n",
    "def postprocess_tokens(tokens):\n",
    "  tokens = tokens.tolist()  # np.array to list[int]\n",
    "  try:  # Remove tokens at and after EOS if any.\n",
    "    eos_pos = tokens.index(tokenizer.eos_id())\n",
    "    tokens = tokens[:eos_pos]\n",
    "  except ValueError:\n",
    "    pass\n",
    "  return tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whzWOojGOtzi"
   },
   "outputs": [],
   "source": [
    "# @title Function to iterate over train and validation examples.\n",
    "SEQLEN = 128\n",
    "\n",
    "# TODO: Consider data iterators skipping big_vision and tf.data?\n",
    "train_dataset = big_vision.datasets.jsonl.DataSource(\n",
    "    os.path.join(DATA_DIR, \"data_train90.jsonl\"),\n",
    "    fopen_keys={\"image\": DATA_DIR})\n",
    "\n",
    "val_dataset = big_vision.datasets.jsonl.DataSource(\n",
    "    os.path.join(DATA_DIR, \"data_val10.jsonl\"),\n",
    "    fopen_keys={\"image\": DATA_DIR})\n",
    "\n",
    "\n",
    "def train_data_iterator(seed=None):\n",
    "  \"\"\"Never ending iterator over training examples.\"\"\"\n",
    "  # Shuffle examples and repeat so one can train for many epochs.\n",
    "  dataset = train_dataset.get_tfdata().shuffle(1_000, seed=seed).repeat()\n",
    "  for example in dataset.as_numpy_iterator():\n",
    "    image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "    image = preprocess_image(image)\n",
    "\n",
    "    prefix = \"caption en\"  # Could also be a different prefix per example.\n",
    "    suffix = example[\"suffix\"].decode().lower()\n",
    "    tokens, mask_ar, mask_loss, _ = preprocess_tokens(prefix, suffix, SEQLEN)\n",
    "\n",
    "    yield {\n",
    "        \"image\": np.asarray(image),\n",
    "        \"text\": np.asarray(tokens),\n",
    "        \"mask_ar\": np.asarray(mask_ar),\n",
    "        \"mask_loss\": np.asarray(mask_loss),\n",
    "    }\n",
    "\n",
    "\n",
    "def validation_data_iterator():\n",
    "  \"\"\"Single iterator over validation examples.\"\"\"\n",
    "  for example in val_dataset.get_tfdata(ordered=True).as_numpy_iterator():\n",
    "    image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "    image = preprocess_image(image)\n",
    "\n",
    "    prefix = \"caption en\"  # Could also be a different prefix per example.\n",
    "    tokens, mask_ar, _, mask_input = preprocess_tokens(prefix, seqlen=SEQLEN)\n",
    "\n",
    "    yield {\n",
    "        \"image\": np.asarray(image),\n",
    "        \"text\": np.asarray(tokens),\n",
    "        \"mask_ar\": np.asarray(mask_ar),\n",
    "        \"mask_input\": np.asarray(mask_input),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "BzJfb5t0nsLq",
    "outputId": "8920b068-c56a-4f99-848d-810fd0cd0068"
   },
   "outputs": [],
   "source": [
    "# @title Inspect training examples.\n",
    "def render_inline(image, resize=(128, 128)):\n",
    "  \"\"\"Convert image into inline html.\"\"\"\n",
    "  image = Image.fromarray(image)\n",
    "  image.resize(resize)\n",
    "  with io.BytesIO() as buffer:\n",
    "    image.save(buffer, format='jpeg')\n",
    "    image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
    "    return f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "def render_example(image, caption):\n",
    "  image = ((image + 1)/2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]\n",
    "  return f\"\"\"\n",
    "    <div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
    "        <img style=\"width:128px; height:128px;\" src=\"{render_inline(image, resize=(64,64))}\" />\n",
    "        <p style=\"width:256px; margin:10px; font-size:small;\">{html.escape(caption)}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_out = \"\"\n",
    "for idx, example in zip(range(8), train_data_iterator()):\n",
    "  caption = postprocess_tokens(example[\"text\"])  # detokenize model input.\n",
    "  caption = caption[len(\"caption en\\n\"):]        # strip prefix\n",
    "  html_out += render_example(example[\"image\"], caption)\n",
    "\n",
    "print(\"Training examples\")\n",
    "display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwUV_imW3WQJ"
   },
   "outputs": [],
   "source": [
    "# @title Define the training step and evaluation loop.\n",
    "#\n",
    "# The main update_fn using AdamW optimizer via Optax.\n",
    "#\n",
    "import optax\n",
    "\n",
    "# Optimizer will be defined in the training loop cell.\n",
    "\n",
    "@functools.partial(jax.jit, donate_argnums=(0, 1))\n",
    "def update_fn(params, opt_state, batch, step, rng):\n",
    "  imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
    "\n",
    "  def loss_fn(params):\n",
    "    # Pass rngs={\"dropout\": rng} to ensure deterministic behavior if dropout is used\n",
    "    text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True, rngs={\"dropout\": rng})\n",
    "    logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
    "\n",
    "    # The model takes as input txts[:, :-1] but the loss is defined as predicting\n",
    "    # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n",
    "    # are part of the loss (e.g. prefix and padded tokens are not included).\n",
    "    mask_loss = batch[\"mask_loss\"][:, 1:]\n",
    "    targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
    "\n",
    "    # Compute the loss per example. i.e. the mean of per token pplx.\n",
    "    # Since each example has a different number of tokens we normalize it.\n",
    "    token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n",
    "    example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n",
    "    example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n",
    "\n",
    "    # batch_loss: mean of per example loss.\n",
    "    return jnp.mean(example_loss)\n",
    "\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "  # Apply gradients to trainable params using AdamW.\n",
    "  # We only want to update trainable parameters.\n",
    "  # Optax updates all parameters by default, so we mask the gradients.\n",
    "  grads = jax.tree.map(lambda g, t: g if t else jnp.zeros_like(g), grads, trainable_mask)\n",
    "  \n",
    "  updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return new_params, new_opt_state, loss\n",
    "\n",
    "# Evaluation/inference loop.\n",
    "def make_predictions(data_iterator, *, num_examples=None,\n",
    "                     batch_size=4, seqlen=SEQLEN, sampler=\"greedy\"):\n",
    "  outputs = []\n",
    "  while True:\n",
    "    # Construct a list of examples in the batch.\n",
    "    examples = []\n",
    "    try:\n",
    "      for _ in range(batch_size):\n",
    "        examples.append(next(data_iterator))\n",
    "        examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n",
    "    except StopIteration:\n",
    "      if len(examples) == 0:\n",
    "        return outputs\n",
    "\n",
    "    # Not enough examples to complete a batch. Pad by repeating last example.\n",
    "    while len(examples) % batch_size:\n",
    "      examples.append(dict(examples[-1]))\n",
    "      examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n",
    "\n",
    "    # Convert list of examples into a dict of np.arrays and load onto devices.\n",
    "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
    "\n",
    "    # Make model predictions\n",
    "    tokens = decode({\"params\": params}, batch=batch,\n",
    "                    max_decode_len=seqlen, sampler=sampler)\n",
    "\n",
    "    # Fetch model predictions to device and detokenize.\n",
    "    tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
    "    tokens = tokens[mask]  # remove padding examples.\n",
    "    responses = [postprocess_tokens(t) for t in tokens]\n",
    "\n",
    "    # Append to html output.\n",
    "    for example, response in zip(examples, responses):\n",
    "      outputs.append((example[\"image\"], response))\n",
    "      if num_examples and len(outputs) >= num_examples:\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "067wj_6bZAG3",
    "outputId": "c3393bab-9c89-410c-9cf6-9e477e194a03"
   },
   "outputs": [],
   "source": [
    "# @title Run training loop.\n",
    "#\n",
    "# Run a short training loop with cosine learning rate schedule.\n",
    "#\n",
    "# Note: the first step can be quite slow on some machines (up to several minutes)\n",
    "# due to XLA compilation of the jax.jit'd function.\n",
    "#\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "TRAIN_STEPS = 30\n",
    "LEARNING_RATE = 3e-5\n",
    "PROFILE_STEP = 10\n",
    "SEED = 42\n",
    "EVAL_TIMES = 1\n",
    "# Enable deterministic operations for reproducibility\n",
    "# jax.config.update(\"jax_deterministic_ops\", True) # Not available in this JAX version\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Initialize JAX RNG key\n",
    "rng = jax.random.PRNGKey(SEED)\n",
    "\n",
    "# Create a timestamped directory for the trace and logs\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "trace_dir = f\"profile/jax-trace_{timestamp}\"\n",
    "log_dir = f\"logs/fit/{timestamp}\"\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "EVAL_STEPS = TRAIN_STEPS // EVAL_TIMES\n",
    "\n",
    "train_data_it = train_data_iterator(seed=SEED)\n",
    "\n",
    "sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
    "    total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,\n",
    "    decay_type=\"cosine\", warmup_percent=0.10)\n",
    "\n",
    "# Define optimizer and initialize state\n",
    "optimizer = optax.adamw(learning_rate=sched_fn, weight_decay=1e-4)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "step_durations = []\n",
    "\n",
    "for step in range(1, TRAIN_STEPS+1):\n",
    "  if step == PROFILE_STEP:\n",
    "    jax.profiler.start_trace(trace_dir)\n",
    "\n",
    "  step_start = time.time()\n",
    "\n",
    "  # Make list of N training examples.\n",
    "  examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "  # Convert list of examples into a dict of np.arrays and load onto devices.\n",
    "  batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "  batch = big_vision.utils.reshard(batch, data_sharding)\n",
    "\n",
    "  # Split RNG key for the current step\n",
    "  rng, step_rng = jax.random.split(rng)\n",
    "\n",
    "  # Training step and report training loss\n",
    "  # Pass opt_state, step, and step_rng to update_fn\n",
    "  params, opt_state, loss = update_fn(params, opt_state, batch, step, step_rng)\n",
    "\n",
    "  # Block until ready to measure accurate GPU time\n",
    "  jax.tree.leaves(params)[0].block_until_ready()\n",
    "  loss = jax.device_get(loss)\n",
    "  \n",
    "  step_end = time.time()\n",
    "\n",
    "  if step == PROFILE_STEP:\n",
    "    jax.profiler.stop_trace()\n",
    "    print(f\"Profiled step {step}. Trace saved to {trace_dir}\")\n",
    "\n",
    "  step_duration = step_end - step_start\n",
    "\n",
    "  if step > 5:\n",
    "    step_durations.append(step_duration)\n",
    "\n",
    "  learning_rate = sched_fn(step)\n",
    "  print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}   time: {step_duration:.2f}s\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  # Write metrics to TensorBoard\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('learning_rate', learning_rate, step=step)\n",
    "    tf.summary.scalar('loss', loss, step=step)\n",
    "    tf.summary.scalar('step_time', step_duration, step=step)\n",
    "    \n",
    "    # Calculate and log images per second per GPU\n",
    "    # Avoid division by zero if step_duration is extremely small (though unlikely with blocking)\n",
    "    if step_duration > 0:\n",
    "      img_per_sec_per_gpu = (BATCH_SIZE / step_duration) / jax.device_count()\n",
    "      tf.summary.scalar('img_per_sec_per_gpu', img_per_sec_per_gpu, step=step)\n",
    "\n",
    "  # if step == 1 or (step % EVAL_STEPS) == 0:\n",
    "  #   print(f\"Model predictions at step {step}\")\n",
    "  #   html_out = \"\"\n",
    "  #   # Batch size must be divisible by the number of devices (8).\n",
    "  #   for image, caption in make_predictions(\n",
    "  #       validation_data_iterator(), num_examples=8, batch_size=8):\n",
    "  #     html_out += render_example(image, caption)\n",
    "  #   display(HTML(html_out))\n",
    "\n",
    "# Wait for the last step to finish to get accurate overall timing\n",
    "jax.tree.leaves(params)[0].block_until_ready()\n",
    "\n",
    "if step_durations:\n",
    "  avg_step_time = sum(step_durations) / len(step_durations)\n",
    "  print(f\"Average step time (after first 5 steps): {avg_step_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hgUhEKjzPdMQ",
    "outputId": "e0f9bfaf-7688-42a9-b1d8-22f7ad5743e6"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # @title Evaluate the model on all examples.\n",
    "# #\n",
    "# # The validation data consists of 10 images in a different domain than training\n",
    "# # data.\n",
    "\n",
    "# print(\"Model predictions\")\n",
    "# html_out = \"\"\n",
    "# # Batch size must be divisible by the number of devices (8).\n",
    "# for image, caption in make_predictions(validation_data_iterator(), batch_size=8):\n",
    "#   html_out += render_example(image, caption)\n",
    "# display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai0NMbAwsr0j"
   },
   "source": [
    "# Save the final checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H_3CV33_JkV"
   },
   "outputs": [],
   "source": [
    "# def npsave(pytree, path):\n",
    "#   names_and_vals, _ = big_vision.utils.tree_flatten_with_names(pytree)\n",
    "#   with open(path, \"wb\") as f:\n",
    "#     np.savez(f, **{k:v for k, v in names_and_vals})\n",
    "\n",
    "# # Takes around 4 minutes\n",
    "# npsave(params, 'my-custom-paligemma-ckpt.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"special\" nature of these two parameters lies in the **mathematical operations** used to compute their gradients, which are fundamentally different from the Matrix Multiplications (GEMMs) used in MLP and Attention layers.\n",
    "\n",
    "Here is the technical breakdown of why these specific layers are resistant to determinism:\n",
    "\n",
    "### 1. `llm/embedder` (The Token Embeddings)\n",
    "*   **Implementation:** In gemma.py, the `Embedder.encode` method uses array indexing: `self.input_embedding_table[(x,)]`.\n",
    "*   **The Operation:** This is a **Gather** operation.\n",
    "*   **The Gradient (The Problem):** The backward pass of a Gather is a **Scatter-Add**.\n",
    "    *   **Why it's special:** Unlike an MLP which updates the whole matrix at once using a dense GEMM, the embedding layer only updates the specific rows corresponding to the tokens in your batch.\n",
    "    *   **Atomic Non-Determinism:** When multiple instances of the same token appear in a batch (e.g., the word \"the\" appears 10 times), the GPU threads responsible for those 10 occurrences all try to add their gradients to the same memory address in the embedding table. They use **Atomic Add** instructions to do this safely.\n",
    "    *   **The Failure:** The order in which these threads win the race to write to memory is random. Because floating-point addition is not associative ($(a+b)+c \\neq a+(b+c)$), this random order results in bit-wise different results.\n",
    "    *   **Why Flags Fail:** While `XLA_FLAGS=--xla_gpu_deterministic_ops=true` is supposed to force a deterministic sorting or reduction, on some hardware/driver combinations (especially with very large vocabularies like Gemma's 256k), the compiler may fall back to the faster atomic path or the deterministic kernel implementation might be incomplete for your specific ROCm version.\n",
    "\n",
    "### 2. `img/embedding/kernel` (The Patch Projection)\n",
    "*   **Implementation:** In vit.py, this is defined as `nn.Conv(..., name=\"embedding\")`.\n",
    "*   **The Operation:** This is a **Convolution**.\n",
    "*   **The Gradient (The Problem):** Calculating the gradient with respect to the convolution *weights* (the kernel) is notoriously difficult to make deterministic.\n",
    "    *   **Why it's special:** To compute the weight gradient, the system effectively convolves the input image with the output gradient.\n",
    "    *   **Algorithm Selection:** Libraries like MIOpen (AMD) or cuDNN (NVIDIA) have dozens of algorithms to do this (GEMM-based, FFT-based, Winograd, Direct Atomic).\n",
    "    *   **Atomic Accumulation:** Many of the fastest algorithms (like Direct) use atomic accumulation to sum up contributions from different parts of the image into the small kernel weights (e.g., 14x14).\n",
    "    *   **The Failure:** Similar to the embedding layer, if the chosen algorithm uses atomic accumulation across the spatial dimensions of the image, the summation order is non-deterministic. If the `TF_CUDNN_DETERMINISTIC` flag is not strictly respected by the specific MIOpen version or if no deterministic algorithm exists for that specific tensor shape, it will remain non-deterministic.\n",
    "\n",
    "### Contrast: Why MLP/Attn are fine\n",
    "*   **Operation:** These layers use `jnp.dot` or `jnp.einsum`.\n",
    "*   **Underlying Kernel:** These map to **Dense GEMM** (General Matrix Multiply).\n",
    "*   **Determinism:** GEMMs use highly structured, tiled reduction patterns (e.g., summing down a fixed column). They generally do not rely on \"race-condition\" style atomic adds to global memory. The summation order is fixed by the tile size and grid structure, making them naturally deterministic or very easy to make so.\n",
    "\n",
    "### Recommendation\n",
    "Since the flags are not resolving the issue for these specific layers on your setup, the only way to achieve bit-wise reproducibility is to **keep these two parameters frozen** (which you likely intended to do anyway for efficient finetuning) or accept the slight numerical noise during training.\n",
    "\n",
    "If you must train them deterministically, you would need to force JAX to use a deterministic implementation of `scatter_add` (e.g., by using `jax.lax.scan` to process updates serially, which is extremely slow) or wait for a driver/XLA update that fixes the deterministic kernel for your specific GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
